# -*- coding: utf-8 -*-
"""health_monitor_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/193UehTzpsQ1vhh9n3L7FN7exRfTdeljh
"""

pip install pandas numpy matplotlib seaborn scikit-learn

pip install statsmodels shap xgboost

pip install streamlit

import os
import warnings
from pathlib import Path
from datetime import datetime, timedelta

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

import xgboost as xgb
import shap
import joblib

import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from statsmodels.tsa.seasonal import STL

customers = pd.read_csv("/content/customer_table.csv")
support = pd.read_csv("/content/customer_support_tickets.csv")
events = pd.read_csv("/content/events.csv")
retail = pd.read_csv("/content/online_retail.csv")
financial = pd.read_csv("/content/synthetic_financial_datasets.csv")
telco = pd.read_excel("/content/telco_customer_churn.xlsx")

# Lowercase & replace spaces with underscores
def clean_columns(df):
    df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")
    return df


customers = clean_columns(customers)
support = clean_columns(support)
financial = clean_columns(financial)
telco = clean_columns(telco)
retail = clean_columns(retail)
events = clean_columns(events)

dfs = [customers, support, events, retail, financial, telco]
dfs = [clean_columns(df) for df in dfs]

# Ensure IDs are strings
for df in [customers, support, retail, telco]:
    if 'customer_id' in df.columns:
        df['customer_id'] = df['customer_id'].astype(str).str.strip()
if 'customerid' in retail.columns:
    retail['customerid'] = retail['customerid'].astype(str).str.strip()

# --- Ensure numeric columns ---
num_cols = ["churn_value", "tenure_months", "monthly_charges", "total_charges", "cltv"]
for col in num_cols:
    telco[col] = pd.to_numeric(telco[col], errors="coerce")
support["first_response_time"] = pd.to_numeric(support["first_response_time"], errors="coerce")
support["time_to_resolution"] = pd.to_numeric(support["time_to_resolution"], errors="coerce")
financial["amount"] = pd.to_numeric(financial["amount"], errors="coerce")

print("Customer IDs standardized")
print("Columns standardized")

"""Merging Tables"""

unified = customers.copy()

unified = unified.merge(
    support,
    on='customer_id',
    how='left',
    suffixes=('', '_support')
)

print("Support tickets merged")

unified = unified.merge(
    telco,
    on='customer_id',
    how='left',
    suffixes=('', '_telco')
)

print("Telco churn data merged")

if 'customerid' in retail.columns:
    unified = unified.merge(
        retail,
        left_on='customer_id',
        right_on='customerid',
        how='left',
        suffixes=('', '_retail')
    )

print("Retail data merged")

if 'visitor_id' in events.columns:
    events['visitor_id'] = events['visitor_id'].astype(str).str.strip()
    event_summary = events.groupby('visitor_id').agg(
        total_events=('event', 'count'),
        unique_items=('item_id', 'nunique'),
        total_transactions=('transaction_id', 'nunique')
    ).reset_index()

    unified = unified.merge(
        event_summary,
        left_on='customer_id',
        right_on='visitor_id',
        how='left'
    )

print("Events data aggregated & merged")

if 'name_orig' in financial.columns:
    financial['name_orig'] = financial['name_orig'].astype(str).str.strip()
    fin_summary = financial.groupby('name_orig').agg(
        total_amount=('amount', 'sum'),
        fraud_count=('isfraud', 'sum')
    ).reset_index()

    unified = unified.merge(
        fin_summary,
        left_on='customer_id',
        right_on='name_orig',
        how='left'
    )

print("Financial data aggregated & merged")

unified.drop_duplicates(subset=['customer_id'], inplace=True)
print(f"Final unified dataset: {len(unified)} customers")

"""KPIs

"""

# Aggregate financial data KPIs
financial_kpis = financial.groupby("customer_id").agg(
    total_transaction_amount=("amount", "sum"),
    avg_transaction_amount=("amount", "mean"),
    transaction_count=("amount", "count"),
    fraud_count=("isfraud", "sum"),
    flagged_fraud_count=("isflaggedfraud", "sum")
).reset_index()

# Calculate fraud rate
financial_kpis["fraud_rate"] = financial_kpis["fraud_count"] / financial_kpis["transaction_count"]

# Convert times to numeric
support["first_response_time"] = pd.to_numeric(support["first_response_time"], errors="coerce")
support["time_to_resolution"] = pd.to_numeric(support["time_to_resolution"], errors="coerce")

# Basic aggregations
support_kpis = support.groupby("customer_id").agg(
    tickets_count=("ticket_id", "count"),
    avg_first_response_time=("first_response_time", "mean"),
    avg_time_to_resolution=("time_to_resolution", "mean"),
    avg_customer_satisfaction=("customer_satisfaction_rating", "mean")
).reset_index()

# Calculate open_ticket_ratio separately
open_ticket_ratio = (
    support.assign(is_open=support["ticket_status"].str.lower() == "open")
    .groupby("customer_id")["is_open"]
    .mean()
    .reset_index()
    .rename(columns={"is_open": "open_ticket_ratio"})
)

# Merge back
support_kpis = support_kpis.merge(open_ticket_ratio, on="customer_id", how="left")

telco_kpis = telco[[
    "customer_id", "tenure_months", "monthly_charges",
    "total_charges", "churn_label", "churn_score", "cltv"
]]

#Events KPIs (per visitor)
events_kpis = events.groupby('visitor_id').agg(
    total_events=('event', 'count'),
    unique_items_viewed=('item_id', 'nunique'),
    total_transactions=('transaction_id', 'nunique')
).reset_index()

# Retail KPIs (overall)
retail['total_price'] = retail['quantity'] *retail['unit_price']
retail_kpis = retail.groupby('customer_id').agg(
    total_orders=('invoice_no', 'nunique'),
    total_quantity=('quantity', 'sum'),
    total_revenue=('total_price', 'sum')
).reset_index()

"""Data Validation"""

events_kpis.describe()

retail_kpis.describe()

support_kpis.describe()

telco_kpis.describe()

financial_kpis.describe()

"""Master Datasets"""

# Customer Health View
customer_health = (
    customers
    .merge(telco_kpis, on="customer_id", how="left")
    .merge(support_kpis, on="customer_id", how="left")
    .merge(financial_kpis, on="customer_id", how="left")
    .merge(retail_kpis, on="customer_id", how="left")
)
# Fraud View
fraud_view = financial[financial["isfraud"] > 0].copy()

# Retail Sales View
retail_sales = retail.copy()

# Events View
events_view = events.copy()



#Export to CSVs
customer_health.to_csv("customer_health.csv", index=False)
fraud_view.to_csv("fraud_view.csv", index=False)
retail_sales.to_csv("retail_sales.csv", index=False)
events_view.to_csv("events_view.csv", index=False)

print("Master datasets created:")
print("customer_health.csv")
print("fraud_view.csv")
print("retail_sales.csv")
print("events_view.csv")

for df_name, df in [
    ("customer_health", customer_health),
    ("fraud_view", fraud_view),
    ("retail_sales", retail_sales),
    ("events_view", events_view)
]:
    print(f"--- {df_name} ---")
    print(df.shape)
    print(df.head(), "\n")

import pandas as pd
import numpy as np

# === 1. Clean up NaNs & data types ===

# Customer Health
customer_health = customer_health.fillna({
    "total_orders": 0,
    "total_quantity": 0,
    "total_revenue": 0,
    "fraud_rate": 0,
    "avg_transaction_amount": 0,
    "transaction_count": 0
})
customer_health["monthly_charges"] = pd.to_numeric(customer_health.get("monthly_charges"), errors="coerce").fillna(0)
customer_health["cltv"] = pd.to_numeric(customer_health.get("cltv"), errors="coerce").fillna(0)

# Fraud View
fraud_view["amount"] = pd.to_numeric(fraud_view["amount"], errors="coerce").fillna(0)
fraud_view["type"] = fraud_view["type"].fillna("Unknown")

# Retail Sales
retail_sales["total_price"] = pd.to_numeric(retail_sales["total_price"], errors="coerce").fillna(0)
retail_sales["quantity"] = pd.to_numeric(retail_sales["quantity"], errors="coerce").fillna(0)
retail_sales["description"] = retail_sales["description"].fillna("Unknown Product")

# Events View
events_view["event"] = events_view["event"].fillna("unknown")
events_view["item_id"] = events_view["item_id"].fillna(-1)


# === 2. Calculate KPIs ===

# Customer Health KPIs
customer_health_kpis = {
    "total_customers": int(customer_health["customer_id"].nunique()),
    "avg_churn_rate": round(customer_health.get("churn_value", pd.Series()).mean(), 4) if "churn_value" in customer_health else np.nan,
    "avg_cltv": round(customer_health["cltv"].mean(), 2),
    "avg_monthly_charges": round(customer_health["monthly_charges"].mean(), 2),
    "avg_orders": round(customer_health["total_orders"].mean(), 2),
    "avg_fraud_rate": round(customer_health["fraud_rate"].mean(), 4)
}

# Fraud KPIs
fraud_kpis = {
    "fraud_tx_count": int(fraud_view.shape[0]),
    "total_fraud_amount": round(fraud_view["amount"].sum(), 2),
    "avg_fraud_amount": round(fraud_view["amount"].mean(), 2),
    "most_common_fraud_type": fraud_view["type"].mode()[0] if not fraud_view.empty else "None"
}

# Retail KPIs
retail_kpis = {
    "total_revenue": round(retail_sales["total_price"].sum(), 2),
    "avg_order_value": round(retail_sales.groupby("invoice_no")["total_price"].sum().mean(), 2),
    "top_product": retail_sales["description"].value_counts().idxmax() if not retail_sales.empty else "None",
    "total_quantity_sold": int(retail_sales["quantity"].sum())
}

# Events KPIs
conversion_rate = (
    events_view[events_view["event"].str.lower() == "transaction"].shape[0] /
    events_view.shape[0]
) if events_view.shape[0] > 0 else 0

events_kpis = {
    "total_events": int(events_view.shape[0]),
    "unique_visitors": int(events_view["visitor_id"].nunique()),
    "conversion_rate": round(conversion_rate, 4),
    "most_viewed_item": events_view["item_id"].value_counts().idxmax() if not events_view.empty else "None"
}

# combine all KPIs into one DataFrame
all_kpis = {
    "customer_health": customer_health_kpis,
    "fraud": fraud_kpis,
    "retail": retail_kpis,
    "events": events_kpis
}

kpi_df = pd.DataFrame(all_kpis)

# === 4. Export to CSV ===
kpi_df.to_csv("kpi_summary.csv")
print("KPI summary exported")

# === KPI CALCULATION FROM MASTER VIEWS ===
from datetime import datetime

# ---------------------------
# 1. Customer Health KPIs
# ---------------------------
if not customer_health.empty:
    total_customers = customer_health['customer_id'].nunique()
    churned_customers = customer_health[customer_health['churn_value'] == 1]['customer_id'].nunique() if 'churn_value' in customer_health else 0

    churn_rate = churned_customers / total_customers if total_customers > 0 else 0
    avg_monthly_charges = customer_health['monthly_charges'].mean() if 'monthly_charges' in customer_health else 0
    fraud_rate = (customer_health['fraud_count'].sum() / customer_health['transaction_count'].sum()) if 'fraud_count' in customer_health and 'transaction_count' in customer_health and customer_health['transaction_count'].sum() > 0 else 0
    total_revenue = customer_health['total_revenue'].sum() if 'total_revenue' in customer_health else 0
else:
    churn_rate = avg_monthly_charges = fraud_rate = total_revenue = 0

# ---------------------------
# 2. Fraud View KPIs
# ---------------------------
if not fraud_view.empty:
    # Ensure datetime column exists
    if 'transaction_date' in fraud_view.columns:
        fraud_view['transaction_date'] = pd.to_datetime(fraud_view['transaction_date'], errors='coerce')
        fraud_per_day = fraud_view.groupby(pd.Grouper(key='transaction_date', freq='D'))['amount'].count().mean()
        fraud_per_week = fraud_view.groupby(pd.Grouper(key='transaction_date', freq='W'))['amount'].count().mean()
        fraud_per_month = fraud_view.groupby(pd.Grouper(key='transaction_date', freq='M'))['amount'].count().mean()
    else:
        fraud_per_day = fraud_per_week = fraud_per_month = 0

    total_transactions = financial.shape[0] if 'financial' in globals() else 0
    fraud_percentage = fraud_view.shape[0] / total_transactions if total_transactions > 0 else 0
else:
    fraud_per_day = fraud_per_week = fraud_per_month = fraud_percentage = 0

# ---------------------------
# 3. Retail Sales KPIs
# ---------------------------
if not retail_sales.empty:
    if 'invoice_date' in retail_sales.columns:
        retail_sales['invoice_date'] = pd.to_datetime(retail_sales['invoice_date'], errors='coerce')
        revenue_per_day = retail_sales.groupby(pd.Grouper(key='invoice_date', freq='D'))['total_price'].sum().mean()
        revenue_per_week = retail_sales.groupby(pd.Grouper(key='invoice_date', freq='W'))['total_price'].sum().mean()
        revenue_per_month = retail_sales.groupby(pd.Grouper(key='invoice_date', freq='M'))['total_price'].sum().mean()
    else:
        revenue_per_day = revenue_per_week = revenue_per_month = 0

    avg_order_value = retail_sales.groupby('invoice_no')['total_price'].sum().mean()
else:
    revenue_per_day = revenue_per_week = revenue_per_month = avg_order_value = 0

# ---------------------------
# 4. Events View KPIs
# ---------------------------
if not events_view.empty:
    # Ensure we have event types
    total_views = events_view[events_view['event'].str.lower() == 'view'].shape[0]
    total_adds_to_cart = events_view[events_view['event'].str.lower() == 'addtocart'].shape[0]
    total_purchases = events_view[events_view['event'].str.lower() == 'transaction'].shape[0]
else:
    total_views = total_adds_to_cart = total_purchases = 0

# ---------------------------
# 5. Aggregate into KPI Summary Table
# ---------------------------
kpi_summary = pd.DataFrame([{
    # Customer Health
    "churn_rate": churn_rate,
    "avg_monthly_charges": avg_monthly_charges,
    "fraud_rate": fraud_rate,
    "total_revenue": total_revenue,

    # Fraud
    "fraud_per_day": fraud_per_day,
    "fraud_per_week": fraud_per_week,
    "fraud_per_month": fraud_per_month,
    "fraud_percentage": fraud_percentage,

    # Retail
    "revenue_per_day": revenue_per_day,
    "revenue_per_week": revenue_per_week,
    "revenue_per_month": revenue_per_month,
    "avg_order_value": avg_order_value,

    # Events
    "total_views": total_views,
    "total_adds_to_cart": total_adds_to_cart,
    "total_purchases": total_purchases
}])

# Save to CSV
kpi_summary.to_csv("kpi_summary_table.csv", index=False)
print("KPI summary table saved: kpi_summary_table.csv")

"""ANOMALIES"""

# === FULL TIME-SERIES ANOMALY DETECTION ===
from statsmodels.tsa.seasonal import STL
from sklearn.ensemble import IsolationForest

def detect_anomalies(df, date_col, value_col, freq='M', contamination=0.05, seasonal=13):
    """
    Detect anomalies in a time-series using STL + IsolationForest.
    Returns a DataFrame with anomaly flags.
    """
    # Ensure datetime
    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
    ts = (
        df.groupby(pd.Grouper(key=date_col, freq=freq))[value_col]
        .sum()
        .reset_index()
        .dropna()
    )

    # Skip if insufficient data
    if len(ts) < seasonal * 2:
        return pd.DataFrame()

    # STL decomposition
    stl = STL(ts[value_col], seasonal=seasonal)
    res = stl.fit()
    ts['residual'] = res.resid

    # Isolation Forest
    iso = IsolationForest(contamination=contamination, random_state=42)
    ts['anomaly_flag'] = iso.fit_predict(ts[['residual']])
    ts['anomaly'] = ts['anomaly_flag'].apply(lambda x: x == -1)

    return ts

# 1. Monthly Revenue
revenue_anomalies = detect_anomalies(
    retail_sales.rename(columns={'invoice_date': 'date'}),
    'date', 'total_price', freq='M'
)

# 2. Monthly Orders
order_anomalies = detect_anomalies(
    retail_sales.rename(columns={'invoice_date': 'date'}),
    'date', 'invoice_no', freq='M'
)

# 3. Average Monthly Churn
if 'churn_value' in customer_health.columns and 'join_date' in customer_health.columns:
    churn_df = customer_health.copy()
    churn_df['join_date'] = pd.to_datetime(churn_df['join_date'], errors='coerce')
    churn_df = churn_df.groupby(pd.Grouper(key='join_date', freq='M'))['churn_value'].mean().reset_index()
    churn_df.rename(columns={'join_date': 'date'}, inplace=True)
    churn_anomalies = detect_anomalies(churn_df, 'date', 'churn_value', freq='M')
else:
    churn_anomalies = pd.DataFrame()

# 4. Average Monthly Support Ticket Resolution Time
if 'ticket_created_date' in support.columns:
    res_df = support.copy()
    res_df['ticket_created_date'] = pd.to_datetime(res_df['ticket_created_date'], errors='coerce')
    res_df = res_df.groupby(pd.Grouper(key='ticket_created_date', freq='M'))['time_to_resolution'].mean().reset_index()
    res_df.rename(columns={'ticket_created_date': 'date'}, inplace=True)
    resolution_anomalies = detect_anomalies(res_df, 'date', 'time_to_resolution', freq='M')
else:
    resolution_anomalies = pd.DataFrame()

# 5. Monthly Fraud Amount
if 'transaction_date' in financial.columns:
    fraud_df = financial.copy()
    fraud_df['transaction_date'] = pd.to_datetime(fraud_df['transaction_date'], errors='coerce')
    fraud_df = fraud_df[fraud_df['isfraud'] > 0]
    fraud_df = fraud_df.groupby(pd.Grouper(key='transaction_date', freq='M'))['amount'].sum().reset_index()
    fraud_df.rename(columns={'transaction_date': 'date'}, inplace=True)
    fraud_anomalies = detect_anomalies(fraud_df, 'date', 'amount', freq='M')
else:
    fraud_anomalies = pd.DataFrame()

# === Export all anomaly tables ===
anomaly_results = {
    'revenue_anomalies.csv': revenue_anomalies,
    'order_anomalies.csv': order_anomalies,
    'churn_anomalies.csv': churn_anomalies,
    'resolution_time_anomalies.csv': resolution_anomalies,
    'fraud_anomalies.csv': fraud_anomalies
}

for filename, df in anomaly_results.items():
    if not df.empty:
        df.to_csv(filename, index=False)
        print(f"Saved anomaly file: {filename}")

print("Customer Health Columns:", customer_health.columns.tolist())

# Convert churn_label to numeric churn_value
if 'churn_label' in customer_health.columns:
    customer_health['churn_value'] = customer_health['churn_label'].map({'Yes': 1, 'No': 0})
    print("churn_value column created from churn_label.")
else:
    print("‚ö† churn_label not found. Cannot create churn_value.")

# === Root Cause Analysis ===
# Pearson correlation with churn_value
if 'churn_value' in customer_health.columns:
    corr_matrix = customer_health.corr(numeric_only=True)
    churn_corr = corr_matrix['churn_value'].sort_values(ascending=False)
    churn_corr.to_csv("churn_correlation.csv")
    print("Correlation analysis saved: churn_correlation.csv")
else:
    print("No churn_value column found for correlation analysis.")

"""Data Quality & Validation System
Automated checks for data completeness, consistency, and accuracy
"""

pip install schedule

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class DataQualityValidator:
    def __init__(self):
        self.quality_report = {}
        self.validation_rules = {}

    def add_validation_rule(self, rule_name, rule_function, threshold=None):
        """Add custom validation rule"""
        self.validation_rules[rule_name] = {
            'function': rule_function,
            'threshold': threshold
        }

    def validate_completeness(self, df, dataset_name, required_columns=None):
        """Check data completeness"""
        report = {
            'dataset': dataset_name,
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'missing_data': {}
        }

        # Overall completeness
        total_cells = df.size
        missing_cells = df.isnull().sum().sum()
        report['overall_completeness'] = round((total_cells - missing_cells) / total_cells * 100, 2)

        # Column-wise completeness
        for col in df.columns:
            missing_count = df[col].isnull().sum()
            completeness = round((len(df) - missing_count) / len(df) * 100, 2)
            report['missing_data'][col] = {
                'missing_count': int(missing_count),
                'completeness_pct': completeness
            }

        # Check required columns
        if required_columns:
            missing_required = [col for col in required_columns if col not in df.columns]
            report['missing_required_columns'] = missing_required

        return report

    def validate_consistency(self, df, dataset_name):
        """Check data consistency"""
        report = {
            'dataset': dataset_name,
            'consistency_checks': {}
        }

        # Duplicate records
        duplicates = df.duplicated().sum()
        report['consistency_checks']['duplicate_rows'] = int(duplicates)

        # Data type consistency
        type_issues = []
        for col in df.columns:
            if df[col].dtype == 'object':
                # Check if numeric columns are stored as strings
                try:
                    pd.to_numeric(df[col], errors='raise')
                    type_issues.append(f"{col}: appears numeric but stored as object")
                except:
                    pass

        report['consistency_checks']['data_type_issues'] = type_issues

        # Value range consistency (for numeric columns)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        range_issues = {}

        for col in numeric_cols:
            col_data = df[col].dropna()
            if len(col_data) > 0:
                q1 = col_data.quantile(0.25)
                q3 = col_data.quantile(0.75)
                iqr = q3 - q1
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr

                outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]
                if len(outliers) > 0:
                    range_issues[col] = {
                        'outlier_count': len(outliers),
                        'outlier_percentage': round(len(outliers) / len(col_data) * 100, 2)
                    }

        report['consistency_checks']['outliers'] = range_issues
        return report

    def validate_accuracy(self, df, dataset_name, business_rules=None):
        """Check data accuracy based on business rules"""
        report = {
            'dataset': dataset_name,
            'accuracy_checks': {}
        }

        # Default business rules
        default_rules = {
            'customer_id_format': lambda x: x.str.match(r'^[A-Za-z0-9]+$').all() if 'customer_id' in df.columns else True,
            'positive_amounts': lambda x: (x >= 0).all() if any('amount' in col.lower() or 'price' in col.lower() or 'revenue' in col.lower() for col in df.columns) else True,
            'valid_email_format': lambda x: x.str.match(r'^[^\s@]+@[^\s@]+\.[^\s@]+$').all() if 'email' in df.columns else True,
            'phone_format': lambda x: x.str.match(r'^\+?[\d\s\-\(\)]+$').all() if 'phone' in df.columns else True
        }

        # Apply default rules
        for rule_name, rule_func in default_rules.items():
            try:
                if rule_name == 'positive_amounts':
                    amount_cols = [col for col in df.columns if any(word in col.lower() for word in ['amount', 'price', 'revenue', 'charges'])]
                    for col in amount_cols:
                        if df[col].dtype in ['int64', 'float64']:
                            result = rule_func(df[col].dropna())
                            report['accuracy_checks'][f'{rule_name}_{col}'] = result
                else:
                    result = rule_func(df)
                    report['accuracy_checks'][rule_name] = result
            except Exception as e:
                report['accuracy_checks'][rule_name] = f"Error: {str(e)}"

        # Apply custom business rules
        if business_rules:
            for rule_name, rule_func in business_rules.items():
                try:
                    result = rule_func(df)
                    report['accuracy_checks'][rule_name] = result
                except Exception as e:
                    report['accuracy_checks'][rule_name] = f"Error: {str(e)}"

        return report

    def validate_timeliness(self, df, dataset_name, date_columns=None):
        """Check data timeliness"""
        report = {
            'dataset': dataset_name,
            'timeliness_checks': {}
        }

        if date_columns:
            for col in date_columns:
                if col in df.columns:
                    try:
                        df[col] = pd.to_datetime(df[col], errors='coerce')

                        # Check for future dates
                        future_dates = df[df[col] > datetime.now()]
                        report['timeliness_checks'][f'{col}_future_dates'] = len(future_dates)

                        # Check data freshness
                        latest_date = df[col].max()
                        days_old = (datetime.now() - latest_date).days if pd.notna(latest_date) else None
                        report['timeliness_checks'][f'{col}_days_since_latest'] = days_old

                    except Exception as e:
                        report['timeliness_checks'][f'{col}_error'] = str(e)

        return report

    def generate_quality_score(self, completeness_report, consistency_report, accuracy_report):
        """Generate overall data quality score"""
        scores = {}

        # Completeness score (0-100)
        completeness_score = completeness_report.get('overall_completeness', 0)
        scores['completeness'] = completeness_score

        # Consistency score
        duplicate_penalty = min(consistency_report['consistency_checks']['duplicate_rows'] * 5, 50)
        outlier_penalty = sum([info['outlier_percentage'] for info in consistency_report['consistency_checks']['outliers'].values()]) / len(consistency_report['consistency_checks']['outliers']) if consistency_report['consistency_checks']['outliers'] else 0
        consistency_score = max(100 - duplicate_penalty - outlier_penalty, 0)
        scores['consistency'] = round(consistency_score, 2)

        # Accuracy score
        accuracy_checks = accuracy_report['accuracy_checks']
        passed_checks = sum([1 for result in accuracy_checks.values() if result == True])
        total_checks = len(accuracy_checks)
        accuracy_score = (passed_checks / total_checks * 100) if total_checks > 0 else 100
        scores['accuracy'] = round(accuracy_score, 2)

        # Overall score (weighted average)
        overall_score = (scores['completeness'] * 0.4 + scores['consistency'] * 0.3 + scores['accuracy'] * 0.3)
        scores['overall'] = round(overall_score, 2)

        return scores

    def run_full_validation(self, datasets_dict, business_rules=None, date_columns=None):
        """Run complete validation suite on multiple datasets"""
        validation_results = {}

        for dataset_name, df in datasets_dict.items():
            print(f"Validating {dataset_name}...")

            # Run all validation checks
            completeness = self.validate_completeness(df, dataset_name)
            consistency = self.validate_consistency(df, dataset_name)
            accuracy = self.validate_accuracy(df, dataset_name, business_rules)
            timeliness = self.validate_timeliness(df, dataset_name, date_columns)

            # Generate quality score
            quality_scores = self.generate_quality_score(completeness, consistency, accuracy)

            validation_results[dataset_name] = {
                'completeness': completeness,
                'consistency': consistency,
                'accuracy': accuracy,
                'timeliness': timeliness,
                'quality_scores': quality_scores,
                'validation_timestamp': datetime.now().isoformat()
            }

        return validation_results

    def export_validation_report(self, validation_results, filename="data_quality_report.html"):
        """Export comprehensive validation report"""
        html_content = """
        <html>
        <head>
            <title>Data Quality Validation Report</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .dataset { margin-bottom: 30px; border: 1px solid #ddd; padding: 15px; }
                .score-excellent { color: green; font-weight: bold; }
                .score-good { color: orange; font-weight: bold; }
                .score-poor { color: red; font-weight: bold; }
                table { border-collapse: collapse; width: 100%; margin-top: 10px; }
                th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
                th { background-color: #f2f2f2; }
            </style>
        </head>
        <body>
        """

        html_content += f"<h1>Data Quality Validation Report</h1>"
        html_content += f"<p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>"

        for dataset_name, results in validation_results.items():
            quality_scores = results['quality_scores']
            overall_score = quality_scores['overall']

            # Determine score class
            if overall_score >= 90:
                score_class = "score-excellent"
            elif overall_score >= 70:
                score_class = "score-good"
            else:
                score_class = "score-poor"

            html_content += f"""
            <div class="dataset">
                <h2>{dataset_name}</h2>
                <p class="{score_class}">Overall Quality Score: {overall_score}%</p>

                <h3>Quality Breakdown</h3>
                <table>
                    <tr><th>Dimension</th><th>Score</th></tr>
                    <tr><td>Completeness</td><td>{quality_scores['completeness']}%</td></tr>
                    <tr><td>Consistency</td><td>{quality_scores['consistency']}%</td></tr>
                    <tr><td>Accuracy</td><td>{quality_scores['accuracy']}%</td></tr>
                </table>

                <h3>Completeness Details</h3>
                <p>Total Rows: {results['completeness']['total_rows']}</p>
                <p>Overall Completeness: {results['completeness']['overall_completeness']}%</p>

                <h3>Consistency Issues</h3>
                <p>Duplicate Rows: {results['consistency']['consistency_checks']['duplicate_rows']}</p>

                <h3>Accuracy Checks</h3>
                <table>
                    <tr><th>Check</th><th>Result</th></tr>
            """

            for check, result in results['accuracy']['accuracy_checks'].items():
                html_content += f"<tr><td>{check}</td><td>{result}</td></tr>"

            html_content += "</table></div>"

        html_content += "</body></html>"

        with open(filename, 'w') as f:
            f.write(html_content)

        print(f"Validation report exported to {filename}")

# Example usage and implementation
if __name__ == "__main__":
    # Load your datasets
    try:
        customer_health = pd.read_csv("customer_health.csv")
        fraud_view = pd.read_csv("fraud_view.csv")
        retail_sales = pd.read_csv("retail_sales.csv")
        events_view = pd.read_csv("events_view.csv")
    except FileNotFoundError:
        print("CSV files not found. Please ensure all master datasets are available.")
        exit()

    # Initialize validator
    validator = DataQualityValidator()

    # Define datasets
    datasets = {
        'customer_health': customer_health,
        'fraud_view': fraud_view,
        'retail_sales': retail_sales,
        'events_view': events_view
    }

    # Define custom business rules
    business_rules = {
        'churn_rate_valid': lambda df: (df['churn_value'] <= 1).all() if 'churn_value' in df.columns else True,
        'tenure_positive': lambda df: (df['tenure_months'] >= 0).all() if 'tenure_months' in df.columns else True,
        'satisfaction_range': lambda df: ((df['customer_satisfaction_rating'] >= 1) & (df['customer_satisfaction_rating'] <= 5)).all() if 'customer_satisfaction_rating' in df.columns else True
    }

    # Define date columns to check
    date_columns = ['invoice_date', 'ticket_created_date', 'transaction_date', 'join_date']

    # Run full validation
    results = validator.run_full_validation(datasets, business_rules, date_columns)

    # Export report
    validator.export_validation_report(results)

    # Save results as JSON for programmatic access
    import json
    with open('validation_results.json', 'w') as f:
        json.dump(results, f, indent=2, default=str)

    print("Data quality validation completed!")
    print("\nQuality Scores Summary:")
    for dataset_name, result in results.items():
        score = result['quality_scores']['overall']
        print(f"{dataset_name}: {score}%")

def extract_kpis_from_data(self):
    """Extract KPIs from loaded KPI summary data"""
    if hasattr(self, 'kpi_summary') and not self.kpi_summary.empty:
        row = self.kpi_summary.iloc[0]  # first row has the KPIs

        self.kpi_data = {
            'total_customers': row.get('total_customers', 0),
            'churn_rate': row.get('avg_churn_rate', 0),
            'avg_cltv': row.get('avg_cltv', 0),
            'fraud_rate': row.get('avg_fraud_rate', 0),
            'total_revenue': row.get('total_revenue', 0),
            'avg_order_value': row.get('avg_order_value', 0),
            'avg_monthly_charges': row.get('avg_monthly_charges', 0),
            'customer_satisfaction': 4.2  # Static until you add this metric
        }

        # Optional extra metrics
        self.extra_metrics = {
            'fraud_tx_count': row.get('fraud_tx_count', 0),
            'total_fraud_amount': row.get('total_fraud_amount', 0),
            'avg_fraud_amount': row.get('avg_fraud_amount', 0),
            'most_common_fraud_type': row.get('most_common_fraud_type', ''),
            'avg_orders': row.get('avg_orders', 0),
            'top_product': row.get('top_product', ''),
            'total_quantity_sold': row.get('total_quantity_sold', 0),
            'total_events': row.get('total_events', 0),
            'unique_visitors': row.get('unique_visitors', 0),
            'conversion_rate': row.get('conversion_rate', 0),
            'most_viewed_item': row.get('most_viewed_item', '')
        }
    else:
        self.create_sample_kpis()

!pip install plotly ipywidgets pandas

import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime

# ===============================
# 1Ô∏è‚É£ Load KPI Summary CSV
# ===============================
def load_kpi_summary(file_path="kpi_summary.csv"):
    df = pd.read_csv(file_path)
    kpi_dict = {}
    for _, row in df.iterrows():
        kpi_name = row.iloc[0]  # First col = KPI name
        kpi_value = row.iloc[1:].dropna().values
        if len(kpi_value) > 0:
            val = kpi_value[0]
            try:
                # Try converting to float if it's numeric
                val = float(val)
            except ValueError:
                pass  # keep as string (e.g., top product name)
            kpi_dict[kpi_name] = val
        else:
            kpi_dict[kpi_name] = None
    return kpi_dict

def fill_missing_churn_rate(kpi_data, customer_health_csv):
    df = pd.read_csv(customer_health_csv)

    # Convert churn_label (Yes/No) to numeric
    if "churn_label" in df.columns:
        df["churn_value"] = df["churn_label"].map({"Yes": 1, "No": 0})
        churn_rate = round(df["churn_value"].mean(), 4)
        kpi_data["avg_churn_rate"] = churn_rate
    else:
        print("‚ö† churn_label column not found, churn rate not updated.")

    return kpi_data



kpi_data = load_kpi_summary("kpi_summary.csv")
kpi_data = fill_missing_churn_rate(kpi_data, "customer_health.csv")

# ===============================
# Create KPI Cards
# ===============================
def create_kpi_cards(kpi_data):
    fig = make_subplots(
        rows=2, cols=4,
        subplot_titles=[
            'Total Customers', 'Avg Churn Rate', 'Avg CLTV', 'Avg Fraud Rate',
            'Total Revenue', 'Avg Order Value', 'Avg Monthly Charges', 'Conversion Rate'
        ],
        specs=[[{'type': 'indicator'}]*4, [{'type': 'indicator'}]*4]
    )

    kpis = [
        ('total_customers', 'number', '#1f77b4'),
        ('avg_churn_rate', 'percent', '#ff7f0e'),
        ('avg_cltv', 'currency', '#2ca02c'),
        ('avg_fraud_rate', 'percent', '#d62728'),
        ('total_revenue', 'currency', '#9467bd'),
        ('avg_order_value', 'currency', '#8c564b'),
        ('avg_monthly_charges', 'currency', '#e377c2'),
        ('conversion_rate', 'percent', '#7f7f7f')
    ]

    row, col = 1, 1
    for kpi, fmt, color in kpis:
        value = kpi_data.get(kpi, 0) or 0
        if fmt == 'percent':
            display_value = value * 100
        else:
            display_value = value

        fig.add_trace(
            go.Indicator(
                mode="number",
                value=display_value,
                number={"font": {"size": 30, "color": color},
                        "suffix": "%" if fmt == 'percent' else ""},
                title={"text": kpi.replace("_", " ").title(), "font": {"size": 14}}
            ),
            row=row, col=col
        )

        col += 1
        if col > 4:
            row += 1
            col = 1

    fig.update_layout(height=500, title_text="Key Performance Indicators")
    return fig

# ===============================
# 3Ô∏è‚É£ Example Trend Data
# ===============================
def generate_sample_trends():
    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']
    return pd.DataFrame({
        'month': months,
        'revenue': np.random.randint(150000, 250000, len(months)),
        'churn_rate': np.random.uniform(0.08, 0.15, len(months)),
        'fraud_rate': np.random.uniform(0.001, 0.003, len(months)),
        'new_customers': np.random.randint(300, 800, len(months)),
        'conversion_rate': np.random.uniform(0.005, 0.012, len(months))
    })

trend_data = generate_sample_trends()

# ===============================
# 4Ô∏è‚É£ Multi-Metric Trends
# ===============================
def create_multi_metric_trend(df):
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('Revenue', 'Churn Rate', 'New Customers', 'Conversion Rate')
    )

    fig.add_trace(go.Scatter(x=df['month'], y=df['revenue'],
                             mode='lines+markers', name='Revenue', line=dict(color='blue')), row=1, col=1)
    fig.add_trace(go.Scatter(x=df['month'], y=df['churn_rate']*100,
                             mode='lines+markers', name='Churn Rate', line=dict(color='red')), row=1, col=2)
    fig.add_trace(go.Bar(x=df['month'], y=df['new_customers'],
                         name='New Customers', marker_color='green'), row=2, col=1)
    fig.add_trace(go.Scatter(x=df['month'], y=df['conversion_rate']*100,
                             mode='lines+markers', name='Conversion Rate', line=dict(color='purple')), row=2, col=2)

    fig.update_layout(height=600, title_text=" Multi-Metric Trends")
    return fig

# ===============================
# 5Ô∏è‚É£ Main Dashboard Display
# ===============================
def display_dashboard():
    kpi_fig = create_kpi_cards(kpi_data)
    kpi_fig.show()

    trend_fig = create_multi_metric_trend(trend_data)
    trend_fig.show()

# ===============================
# ‚ñ∂ Run Dashboard
# ===============================
if __name__ == "__main__":
    display_dashboard()

import pandas as pd

# Load data
customer_health = pd.read_csv("customer_health.csv")

# Calculate avg churn score
avg_churn_score = customer_health["churn_score"].mean()

# Load KPI summary
kpi_summary = pd.read_csv("kpi_summary.csv", index_col=0)

# Update the value
kpi_summary.loc["avg_churn_score", "customer_health"] = avg_churn_score

# Load datasets
customer_health = pd.read_csv("customer_health.csv")
fraud_view = pd.read_csv("fraud_view.csv")
retail_sales = pd.read_csv("retail_sales.csv")
events_view = pd.read_csv("events_view.csv")

# --- CUSTOMER KPIs ---
kpis = {
    "total_customers": len(customer_health),
    "avg_churn_rate": customer_health["churn_score"].mean(),  # numeric churn score
    "avg_cltv": customer_health["cltv"].mean(),
    "avg_monthly_charges": customer_health["monthly_charges"].mean(),
    "avg_orders": customer_health["total_orders"].mean(),
}

# --- FRAUD KPIs ---
kpis.update({
    "avg_fraud_rate": customer_health["fraud_rate"].mean(),
    "fraud_tx_count": fraud_view.shape[0],
    "total_fraud_amount": fraud_view["amount"].sum(),
    "avg_fraud_amount": fraud_view["amount"].mean(),
    "most_common_fraud_type": fraud_view["type"].mode()[0] if not fraud_view.empty else None,
})

# --- RETAIL KPIs ---
kpis.update({
    "total_revenue": retail_sales["total_price"].sum(),
    "avg_order_value": retail_sales["total_price"].mean(),
    "top_product": retail_sales["description"].mode()[0] if not retail_sales.empty else None,
    "total_quantity_sold": retail_sales["quantity"].sum(),
})

# --- EVENTS KPIs ---
kpis.update({
    "total_events": events_view.shape[0],
    "unique_visitors": events_view["visitor_id"].nunique(),
    "conversion_rate": retail_sales["customer_id"].nunique() / events_view["visitor_id"].nunique(),
    "most_viewed_item": events_view["item_id"].mode()[0] if not events_view.empty else None,
})

# Save KPI summary as DataFrame (one row, each KPI a column)
kpi_df = pd.DataFrame([kpis])
kpi_df.to_csv("kpi_summary.csv", index=False)

print("KPI summary updated and saved to kpi_summary.csv")

!pip install streamlit pyngrok
!streamlit run dashboard.py & npx localtunnel --port 8501

import pandas as pd
from sklearn.ensemble import IsolationForest

# Load KPI summary
kpi_df = pd.read_csv("kpi_summary.csv", index_col=0)

# Transpose so KPIs are columns
kpi_df = kpi_df.T.reset_index(drop=True)

# Convert all possible values to numeric
numeric_kpis = kpi_df.apply(pd.to_numeric, errors="coerce")

# Drop any columns that are still completely NaN
numeric_kpis = numeric_kpis.dropna(axis=1, how="all")

# Fill remaining NaNs with 0 for anomaly detection
numeric_kpis = numeric_kpis.fillna(0)

# Isolation Forest
model = IsolationForest(contamination=0.05, random_state=42)
kpi_df["anomaly_flag"] = model.fit_predict(numeric_kpis)

print(kpi_df.head())

import pandas as pd
from sklearn.ensemble import IsolationForest

# Load KPI summary
kpi_df = pd.read_csv("kpi_summary.csv")

# Convert all columns (except the identifier ones) to numeric where possible
for col in kpi_df.columns:
    kpi_df[col] = pd.to_numeric(kpi_df[col], errors="coerce")

# Drop completely empty columns
kpi_df = kpi_df.dropna(axis=1, how="all")

# Keep only numeric KPIs
numeric_kpis = kpi_df.select_dtypes(include=['number'])

if numeric_kpis.empty:
    raise ValueError("No numeric columns found in KPI summary for anomaly detection.")

# Isolation Forest
model = IsolationForest(contamination=0.05, random_state=42)
kpi_df["anomaly_flag"] = model.fit_predict(numeric_kpis)

# Rolling average anomaly check (simple)
window = 3
rolling_mean = numeric_kpis.rolling(window).mean()
rolling_std = numeric_kpis.rolling(window).std()

# Flag anomalies if deviation > 2 std deviations
for col in numeric_kpis.columns:
    kpi_df[f"{col}_rolling_anomaly"] = abs(numeric_kpis[col] - rolling_mean[col]) > (2 * rolling_std[col])

# Save results
kpi_df.to_csv("kpi_anomalies.csv", index=False)
print("‚úÖ Anomaly detection completed and saved to kpi_anomalies.csv")
print(kpi_df.head())

import seaborn as sns
import matplotlib.pyplot as plt

# Load detailed customer health data
customer_health = pd.read_csv("customer_health.csv")

# Keep only numeric columns
numeric_cols = customer_health.select_dtypes(include=['number'])

# Correlation matrix
corr = numeric_cols.corr()

# Plot heatmap
plt.figure(figsize=(18, 10))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation Matrix - Root Cause Analysis")
plt.show()

# Save correlation
corr.to_csv("kpi_correlations.csv")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import joblib

df = pd.read_csv("customer_health.csv")
df.columns = df.columns.str.lower()



# Preprocess Data
# dropping irrelevant columns
drop_cols = ['customer_id', 'customer_name', 'customer_email', 'country', 'state', 'city']
df = df.drop(columns=drop_cols, errors='ignore')

# Handle missing values (fill numeric with median, categorical with mode)
for col in df.select_dtypes(include=['float64', 'int64']):
    df[col] = df[col].fillna(df[col].median())

for col in df.select_dtypes(include=['object']):
    df[col] = df[col].fillna(df[col].mode()[0])

# Encode churn_label (Yes=1, No=0)
if 'churn_label' in df.columns:
    df['churn_label'] = df['churn_label'].str.strip().str.lower().map({'yes': 1, 'no': 0})



# 4. Feature/Target Split
target = 'churn_label'
features = df.drop(columns=[target], errors='ignore')
X = pd.get_dummies(features)  # One-hot encode categoricals
y = df[target]


# 5. Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)


# 6. Train Model
model = RandomForestClassifier(
    n_estimators=200,
    max_depth=10,
    random_state=42
)
model.fit(X_train, y_train)


# 7. Evaluate Model
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print(f"ROC-AUC Score: {roc_auc_score(y_test, y_prob):.4f}")

joblib.dump(model, "churn_model.pkl")

pred_df = X_test.copy()
pred_df['actual_churn'] = y_test
pred_df['predicted_churn'] = y_pred
pred_df['churn_probability'] = y_prob
pred_df.to_csv("churn_predictions.csv", index=False)

print(" Model trained, saved as churn_model.pkl, predictions saved to churn_predictions.csv")

try:
    print(model.feature_names_in_)
except AttributeError:
    print("Model has no attribute feature_names_in_")

print(df.head())

import pandas as pd
import shap
import joblib

model = joblib.load("churn_model.pkl")
df = pd.read_csv("X_train.csv")

#One-hot encoding categorical columns
categorical_cols = ['senior_citizen', 'customer_gender', 'partner']
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=False)

feature_cols = [
    'customer_age', 'latitude', 'longitude', 'tenure_months', 'monthly_charges',
    'total_charges', 'churn_score', 'cltv', 'tickets_count',
    'avg_first_response_time', 'avg_time_to_resolution',
    'avg_customer_satisfaction', 'open_ticket_ratio',
    'total_transaction_amount', 'avg_transaction_amount', 'transaction_count',
    'fraud_count', 'flagged_fraud_count', 'fraud_rate', 'total_orders',
    'total_quantity', 'total_revenue', 'senior_citizen_no', 'senior_citizen_yes',
    'customer_gender_Female', 'customer_gender_Male', 'partner_No',
    'partner_Yes'
]

missing_cols = [col for col in feature_cols if col not in df_encoded.columns]
if missing_cols:
    print("Warning! Missing columns after encoding:", missing_cols)

# 6Ô∏è‚É£ Select features
X = df_encoded[feature_cols]

# 7Ô∏è‚É£ Extract customer IDs for reference
customer_ids = df['customer_id']

churn_preds = pd.read_csv("churn_predictions.csv")

# üßê Inspect columns
print("Churn predictions columns:", churn_preds.columns.tolist())

# Preview first rows
print(churn_preds.head())

print("Churn predictions columns:", churn_preds.columns.tolist())

X_numeric = X.astype(float)

# Sample background from numeric data
background = X_numeric.sample(100, random_state=42)

# Create explainer
explainer = shap.TreeExplainer(model, data=background, feature_perturbation="interventional")

# Calculate SHAP values
shap_values = explainer.shap_values(X_numeric)

if isinstance(shap_values, list):
    shap_values = shap_values[1]

batch_size = 500
shap_values_list = []

for i in range(0, len(X), batch_size):
    batch = X.iloc[i:i+batch_size]
    batch_shap_values = explainer.shap_values(batch)
    if isinstance(batch_shap_values, list):
        batch_shap_values = batch_shap_values[1]
    shap_values_list.append(batch_shap_values)

import numpy as np
shap_values = np.vstack(shap_values_list)

# Load churn predictions without IDs
churn_preds = pd.read_csv("churn_predictions.csv")

# Load the dataset used for prediction (must have customer_id in same order)
full_df = pd.read_csv("customer_health.csv")  # or your preprocessed model input data

# Attach customer_id
churn_preds["customer_id"] = full_df["customer_id"]

# Save back
churn_preds.to_csv("churn_predictions.csv", index=False)

import pandas as pd

# Ensure customer_id is a real column (not an index)
customer_health = customer_health.reset_index(drop=True)
churn_preds = churn_preds.reset_index(drop=True)

# Clean column names (strip spaces, lowercase)
customer_health.columns = customer_health.columns.str.strip().str.lower()
churn_preds.columns = churn_preds.columns.str.strip().str.lower()

if "customer_id" not in customer_health.columns:
    raise KeyError("customer_id not found in customer_health")
else: print("customer_id found in customer_health")
if "customer_id" not in churn_preds.columns:
    raise KeyError("customer_id not found in churn_preds")
else: print("customer_id found in churn_preds")

merged_df = pd.merge(customer_health, churn_preds, on="customer_id", how="left")

print("‚úÖ Merge successful! Shape:", merged_df.shape)

merged_df.to_csv("customer_health_with_churn.csv", index=False)
print("Saved merged dataset to customer_health_with_churn.csv")

# prepare_dashboard_data.py
import pandas as pd
import numpy as np
import os

def safe_read_csv(path, **kwargs):
    if not os.path.exists(path):
        print(f" File not found: {path}")
        return pd.DataFrame()
    return pd.read_csv(path, **kwargs)

def normalize_cols(df):
    df = df.copy()
    df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")
    return df

# 1. Load files (expected names)
customer_health = safe_read_csv("customer_health.csv")
churn_preds = safe_read_csv("churn_predictions.csv")            # expected columns: customer_id, predicted_churn, churn_probability or churn_probability
kpi_anomalies = safe_read_csv("kpi_anomalies.csv")              # single-row KPI anomalies or time-series anomalies
kpi_summary = safe_read_csv("kpi_summary.csv", index_col=None)  # user's KPI summary (kpi name as first column)
shap_importance = safe_read_csv("shap_importance.csv")          # optional: feature, importance

# 2. Normalize column names and ensure customer_id presence
for df_name, df in [("customer_health", customer_health), ("churn_preds", churn_preds),
                    ("kpi_anomalies", kpi_anomalies), ("kpi_summary", kpi_summary), ("shap_importance", shap_importance)]:
    if df is not None and not df.empty:
        vars()[df_name] = normalize_cols(df)

# 3. Ensure customer_id exists and is string
if not customer_health.empty:
    if 'customer_id' not in customer_health.columns and 'customerid' in customer_health.columns:
        customer_health = customer_health.rename(columns={'customerid': 'customer_id'})
    if 'customer_id' not in customer_health.columns:
        raise KeyError("customer_health must contain customer_id column")
    customer_health['customer_id'] = customer_health['customer_id'].astype(str).str.strip()

# churn_preds adjustments
if not churn_preds.empty:
    if 'customer_id' not in churn_preds.columns:
        # try to infer from index
        churn_preds = churn_preds.reset_index().rename(columns={'index':'customer_id'})
    churn_preds['customer_id'] = churn_preds['customer_id'].astype(str).str.strip()
    # Try to standardize churn probability column names
    churn_prob_cols = ['churn_probability', 'probability', 'churn_prob', 'prob']
    churn_prob_col = None
    for c in churn_prob_cols:
        if c in churn_preds.columns:
            churn_prob_col = c
            break
    if churn_prob_col is None:
        # maybe model predicted_proba stored as 'churn_score' (0-100)
        if 'churn_score' in churn_preds.columns:
            churn_preds['churn_prob'] = pd.to_numeric(churn_preds['churn_score'], errors='coerce') / 100.0
            churn_prob_col = 'churn_prob'
        else:
            # try 'predicted_churn' where numeric indicates prob
            if 'predicted_churn' in churn_preds.columns and churn_preds['predicted_churn'].dtype != object:
                churn_preds['churn_prob'] = pd.to_numeric(churn_preds['predicted_churn'], errors='coerce')
                churn_prob_col = 'churn_prob'
    # create consistent columns
    if churn_prob_col:
        churn_preds['churn_probability'] = pd.to_numeric(churn_preds[churn_prob_col], errors='coerce')
    # also create predicted label if not present
    if 'predicted_label' not in churn_preds.columns:
        if 'predicted_churn' in churn_preds.columns:
            # if predicted_churn is 0/1 or yes/no
            try:
                churn_preds['predicted_label'] = churn_preds['predicted_churn'].astype(int)
            except:
                churn_preds['predicted_label'] = churn_preds['predicted_churn'].map({'yes':1,'no':0,'Yes':1,'No':0})
        elif 'churn_probability' in churn_preds.columns:
            churn_preds['predicted_label'] = (churn_preds['churn_probability'] >= 0.5).astype(int)

# 4. KPI summary flattening: user format has KPI names in first column and categories as columns (customer_health, fraud, retail, events)
def flatten_kpi_summary(df):
    if df.empty:
        return {}
    df2 = df.copy()
    # If first column is KPI names and next columns are categories:
    # Some formats: first column header maybe unnamed. We handle generically.
    # We'll take first column values as keys; the rest we flatten into a dict with keys f"{kpi}" optionally namespaced by category.
    first_col = df2.columns[0]
    result = {}
    for _, row in df2.iterrows():
        kpi_name = str(row[first_col]).strip()
        # take the first non-null cell after the first column (prefer customer_health column if exists)
        value = None
        # common column names we prefer: customer_health, fraud, retail, events
        prefer_cols = ['customer_health', 'fraud', 'retail', 'events']
        for c in prefer_cols:
            if c in df2.columns and pd.notna(row[c]):
                value = row[c]
                break
        if value is None:
            # fallback: first non-null value across the row (excluding first col)
            for c in df2.columns[1:]:
                if pd.notna(row[c]):
                    value = row[c]
                    break
        result[kpi_name] = value
    return result

kpi_flat = flatten_kpi_summary(kpi_summary)

# 5. If shap_importance not present, create a simple placeholder from numeric correlations
if shap_importance.empty and not customer_health.empty:
    # estimate simple importance by correlation with churn_score/churn_value/churn_probability
    numeric = customer_health.select_dtypes(include=[np.number]).copy()
    target_col = None
    for t in ['churn_score','churn_value','churn_probability','churn']:
        if t in numeric.columns:
            target_col = t
            break
    if target_col:
        corrs = numeric.corr().get(target_col, pd.Series()).abs().sort_values(ascending=False)
        corrs = corrs.drop(target_col, errors='ignore').head(30)
        shap_importance = pd.DataFrame({
            'feature': corrs.index,
            'importance': corrs.values
        }).reset_index(drop=True)
        print("Generated approximate shap_importance from correlations.")
    else:
        shap_importance = pd.DataFrame({'feature':[], 'importance':[]})

# 6. Merge all into one master table
# We'll join churn_preds onto customer_health by customer_id, then attach last-known anomaly flags (kpi_anomalies)
master = customer_health.copy() if not customer_health.empty else pd.DataFrame()

if not churn_preds.empty:
    master = master.merge(churn_preds[['customer_id','churn_probability','predicted_label']], on='customer_id', how='left')

# attach anomalies: kpi_anomalies could be wide or timeseries; for small summary table assume single-row with columns like total_revenue_rolling_anomaly
if not kpi_anomalies.empty:
    ka = kpi_anomalies.copy()
    # If kpi_anomalies looks like one row wide with columns, attach columns as suffix to master (same values)
    if ka.shape[0] == 1:
        # broadcast each anomaly column to master as a column (prefix anomaly_)
        for col in ka.columns:
            master[f"anomaly_{col}"] = ka.iloc[0][col]
    else:
        # if it's time series of KPI anomalies, take the last row
        last = ka.tail(1).iloc[0]
        for col in ka.columns:
            master[f"anomaly_{col}"] = last[col]

# 7. Save merged outputs for the dashboard and Tableau
os.makedirs("output", exist_ok=True)
master.to_csv("output/dashboard_master.csv", index=False)
pd.DataFrame([kpi_flat]).to_csv("output/kpi_summary_flat.csv", index=False)
shap_importance.to_csv("output/shap_importance.csv", index=False)

print("Created files in ./output:")
print(" - dashboard_master.csv")
print(" - kpi_summary_flat.csv")
print(" - shap_importance.csv")

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from pathlib import Path

DATA_DIR = Path("output")
MASTER_PATH = DATA_DIR / "dashboard_master.csv"
KPI_SUMMARY_PATH = DATA_DIR / "kpi_summary_flat.csv"
SHAP_PATH = DATA_DIR / "shap_importance.csv"

st.set_page_config(layout="wide", page_title="Auto-Pilot Business Health Monitor")

st.title(" Auto-Pilot Business Health Monitor ‚Äî Executive View")

@st.cache_data
def load_files():
    master = pd.read_csv(MASTER_PATH) if MASTER_PATH.exists() else pd.DataFrame()
    kpi = pd.read_csv(KPI_SUMMARY_PATH) if KPI_SUMMARY_PATH.exists() else pd.DataFrame()
    shap = pd.read_csv(SHAP_PATH) if SHAP_PATH.exists() else pd.DataFrame()
    return master, kpi, shap

master, kpi_flat_df, shap_df = load_files()

# ---------------------
# KPI Summary (top row)
# ---------------------
st.header("KPI Summary")
if not kpi_flat_df.empty:
    kpi_dict = kpi_flat_df.iloc[0].to_dict()
else:
    kpi_dict = {}

# helper to safely parse numbers
def safe_float(v):
    try:
        return float(v)
    except:
        return None

col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("Total Customers", int(safe_float(kpi_dict.get("total_customers", 0)) or 0))
    st.metric("Avg CLTV", f"${safe_float(kpi_dict.get('avg_cltv', 0)) or 0:,.0f}")
with col2:
    churn_rate = safe_float(kpi_dict.get("avg_churn_rate", None))
    if churn_rate is None:
        # try reading churn_score average from master
        if 'churn_score' in master.columns:
            churn_rate = master['churn_score'].mean() / 100.0
        elif 'churn_value' in master.columns:
            churn_rate = master['churn_value'].mean()
        else:
            churn_rate = 0.0
    st.metric("Avg Churn Rate", f"{churn_rate*100:.2f}%")
    st.metric("Avg Monthly Charges", f"${safe_float(kpi_dict.get('avg_monthly_charges', 0)) or 0:,.2f}")
with col3:
    st.metric("Total Revenue", f"${safe_float(kpi_dict.get('total_revenue', 0)) or 0:,.0f}")
    st.metric("Avg Order Value", f"${safe_float(kpi_dict.get('avg_order_value', 0)) or 0:,.2f}")
with col4:
    st.metric("Fraud Rate", f"{safe_float(kpi_dict.get('avg_fraud_rate', 0)) or 0:.4f}")
    st.metric("Total Fraud Amount", f"${safe_float(kpi_dict.get('total_fraud_amount', 0)) or 0:,.0f}")

st.markdown("---")

# ---------------------
# Anomaly Alerts
# ---------------------
st.subheader("Anomaly Alerts")
if master.empty:
    st.info("No master dataset found. Run prepare_dashboard_data.py first.")
else:
    # Find columns that begin with 'anomaly_' and show non-empty ones
    anomaly_cols = [c for c in master.columns if c.startswith("anomaly_")]
    if anomaly_cols:
        # create summary of anomalies across customers
        anomaly_summary = {}
        for col in anomaly_cols:
            vals = master[col].dropna()
            # if anomaly column is a boolean or numeric flag
            if vals.size > 0:
                anomaly_summary[col] = int(vals.astype(bool).sum())
        if anomaly_summary:
            anom_df = pd.DataFrame(list(anomaly_summary.items()), columns=["anomaly", "count"])
            st.table(anom_df)
        else:
            st.write("No anomalies flagged in master table.")
    else:
        st.write("No anomaly columns found in master data. Check kpi_anomalies input.")

st.markdown("---")

# ---------------------
# Churn Risk Leaderboard
# ---------------------
st.subheader("Churn Risk Leaderboard")
if 'churn_probability' in master.columns:
    topn = st.slider("Top N high-risk customers", 5, 50, 10)
    # sort by churn_probability descending (higher = more likely to churn)
    leaderboard = master[['customer_id','customer_name'] + [c for c in ['churn_probability','churn_score','predicted_label'] if c in master.columns]].copy()
    leaderboard['churn_probability'] = pd.to_numeric(leaderboard.get('churn_probability',0), errors='coerce').fillna(0)
    leaderboard = leaderboard.sort_values('churn_probability', ascending=False).head(topn)
    st.dataframe(leaderboard.reset_index(drop=True))
    # plot distribution
    fig = px.histogram(master, x='churn_probability', nbins=25, title="Churn probability distribution")
    st.plotly_chart(fig, use_container_width=True)
else:
    st.info("No churn_probability column found in master. Ensure churn_predictions.csv has churn_probability or churn_score.")

st.markdown("---")

# ---------------------
# Root Cause: SHAP / Feature importances
# ---------------------
st.subheader("Root Cause (feature importance / SHAP summary)")
if not shap_df.empty:
    shap_df2 = shap_df.sort_values("importance", ascending=False).head(30)
    fig = px.bar(shap_df2, x='importance', y='feature', orientation='h', title="Top features influencing churn", width=900, height=600)
    fig.update_layout(yaxis={'categoryorder':'total ascending'})
    st.plotly_chart(fig, use_container_width=True)
else:
    st.write("No shap_importance.csv found. If you have a shap output, save as output/shap_importance.csv (feature,importance).")
    # fallback: show correlation-based importances if available
    if not master.empty:
        numeric = master.select_dtypes(include=[np.number])
        target = None
        for t in ['churn_score','churn_value','churn_probability']:
            if t in numeric.columns: target = t; break
        if target:
            corr = numeric.corr()[target].abs().sort_values(ascending=False).drop(target, errors='ignore').head(20)
            fig = px.bar(x=corr.values, y=corr.index, orientation='h', labels={'x':'abs(correlation)','y':'feature'}, title='Approx feature importance (abs corr with churn)')
            st.plotly_chart(fig, use_container_width=True)

st.markdown("---")

# ---------------------
# Download / Export controls for Tableau
# ---------------------
st.subheader("Export for Tableau")
st.write("You can download the master CSVs and connect them in Tableau (File ‚Üí Open ‚Üí CSV).")
col_a, col_b = st.columns(2)
with col_a:
    if master is not None and not master.empty:
        st.download_button("Download dashboard_master.csv", master.to_csv(index=False).encode('utf-8'), file_name="dashboard_master.csv")
with col_b:
    if kpi_flat_df is not None and not kpi_flat_df.empty:
        st.download_button("Download kpi_summary_flat.csv", kpi_flat_df.to_csv(index=False).encode('utf-8'), file_name="kpi_summary_flat.csv")

st.caption("Tip: In Tableau, use the master CSV as your primary data source; use kpi_summary_flat.csv + shap_importance.csv as supporting extracts. Schedule refreshes if your environment supports it.")